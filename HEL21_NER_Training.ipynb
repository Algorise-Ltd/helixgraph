{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEL-21 NER Model Training on Google Colab\n",
    "\n",
    "This notebook trains the Named Entity Recognition model for the HelixGraph project.\n",
    "\n",
    "**Training Data:**\n",
    "- 680 training examples\n",
    "- 170 validation examples\n",
    "- 8 entity types: SUPPLIER, PRODUCT, CAMPAIGN, CONTRACT, PO, INVOICE, ROLE, SKILL\n",
    "\n",
    "**Model:** RoBERTa-base transformer with spaCy\n",
    "\n",
    "**Expected Training Time:** \n",
    "- CPU: 2-3 hours\n",
    "- GPU (T4): 30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to project directory\n",
    "PROJECT_PATH = '/content/drive/MyDrive/Helixgraph'\n",
    "\n",
    "# Create project directory if it doesn't exist\n",
    "!mkdir -p \"$PROJECT_PATH\"\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted\")\n",
    "print(f\"üìÅ Project path: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy with transformers support\n",
    "!pip install -U spacy[transformers]\n",
    "\n",
    "# Download RoBERTa model\n",
    "!python -m spacy download en_core_web_trf\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    use_gpu = 0\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU available, will use CPU (slower)\")\n",
    "    print(\"   To enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    use_gpu = -1\n",
    "\n",
    "print(f\"\\nüéØ Training will use: {'GPU' if use_gpu >= 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Training Files\n",
    "\n",
    "All files are already synced to Google Drive! Let's verify they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all files are in Google Drive\n",
    "print(\"üìÅ Checking training files...\\n\")\n",
    "\n",
    "# Check training data\n",
    "print(\"Training Data:\")\n",
    "!ls -lh \"$PROJECT_PATH/nlp/training_data/spacy/\"\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "!ls -lh \"$PROJECT_PATH/nlp/configs/\"\n",
    "\n",
    "print(\"\\n‚úÖ All files are ready in Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Verification\n",
    "\n",
    "Let's double-check all required files are present and ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check all required files\n",
    "required_files = [\n",
    "    f\"{PROJECT_PATH}/nlp/training_data/spacy/train.spacy\",\n",
    "    f\"{PROJECT_PATH}/nlp/training_data/spacy/dev.spacy\",\n",
    "    f\"{PROJECT_PATH}/nlp/configs/config.cfg\"\n",
    "]\n",
    "\n",
    "print(\"üìã Checking required files:\\n\")\n",
    "all_present = True\n",
    "for filepath in required_files:\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath) / 1024\n",
    "        print(f\"‚úÖ {filepath} ({size:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {filepath} (MISSING)\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\nüéâ All files present! Ready to train.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some files are missing. Please upload them first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training! üöÄ\n",
    "\n",
    "This will take 30-45 minutes on GPU, or 2-3 hours on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Create output directory\n",
    "!mkdir -p nlp/models/ner_model\n",
    "\n",
    "# Start training\n",
    "print(\"üöÄ Starting NER model training...\\n\")\n",
    "print(f\"   Using: {'GPU' if use_gpu >= 0 else 'CPU'}\")\n",
    "print(f\"   Config: {PROJECT_PATH}/nlp/configs/config.cfg\")\n",
    "print(f\"   Output: {PROJECT_PATH}/nlp/models/ner_model\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python -m spacy train \\\n",
    "    nlp/configs/config.cfg \\\n",
    "    --output nlp/models/ner_model \\\n",
    "    --paths.train nlp/training_data/spacy/train.spacy \\\n",
    "    --paths.dev nlp/training_data/spacy/dev.spacy \\\n",
    "    --gpu-id $use_gpu\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Evaluating model on dev set...\\n\")\n",
    "\n",
    "!python -m spacy evaluate \\\n",
    "    nlp/models/ner_model/model-best \\\n",
    "    nlp/training_data/spacy/dev.spacy \\\n",
    "    --gpu-id $use_gpu\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Model with Sample Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load trained model\n",
    "print(\"üì¶ Loading trained model...\")\n",
    "nlp = spacy.load(f\"{PROJECT_PATH}/nlp/models/ner_model/model-best\")\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"The Marketing Coordinator managed the Nike Summer Sale campaign successfully.\",\n",
    "    \"Invoice INV-123456 from Tech Suppliers Ltd was paid via PO-789012 last month.\",\n",
    "    \"Our Software Engineer with Python expertise led the campaign optimization project.\",\n",
    "    \"Contract CTR-445566 with Global Procurement Inc covers delivery of office supplies.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with sample sentences:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, text in enumerate(test_sentences, 1):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(f\"\\n{i}. {text}\")\n",
    "    print(f\"   Entities found: {len(doc.ents)}\")\n",
    "    \n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(f\"     - [{ent.label_}] '{ent.text}'\")\n",
    "    else:\n",
    "        print(\"     (No entities detected)\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entities in a sample sentence\n",
    "sample_text = \"The Product Manager with SQL and Leadership skills managed the Apple iPhone Launch campaign and approved invoice INV-998877 from Tech Solutions via PO-112233.\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(\"üé® Entity Visualization:\\n\")\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "print(\"\\nüìã Detected Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"   [{ent.label_:12}] {ent.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: View Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if training metrics file exists\n",
    "metrics_file = f\"{PROJECT_PATH}/nlp/models/ner_model/model-best/meta.json\"\n",
    "\n",
    "if os.path.exists(metrics_file):\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    print(\"üìä Training Metrics:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'performance' in meta:\n",
    "        perf = meta['performance']\n",
    "        print(f\"\\nüéØ Overall Performance:\")\n",
    "        for key, value in perf.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   {key:20} : {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚úÖ Model saved to: {PROJECT_PATH}/nlp/models/ner_model/model-best\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Metrics file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Download Trained Model (Optional)\n",
    "\n",
    "Download the trained model to your local machine for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "model_path = f\"{PROJECT_PATH}/nlp/models/ner_model/model-best\"\n",
    "zip_path = f\"{PROJECT_PATH}/ner_model_trained.zip\"\n",
    "\n",
    "print(\"üì¶ Creating zip file...\")\n",
    "!cd \"$PROJECT_PATH\" && zip -r ner_model_trained.zip nlp/models/ner_model/model-best/\n",
    "\n",
    "print(\"\\nüì• Download the model:\")\n",
    "files.download(zip_path)\n",
    "\n",
    "print(\"‚úÖ Model downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ Training Complete!\n",
    "\n",
    "Your NER model has been trained successfully on Google Colab.\n",
    "\n",
    "**Model Location:** `{PROJECT_PATH}/nlp/models/ner_model/model-best`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Review evaluation metrics above\n",
    "2. Test with your own sentences\n",
    "3. Download model for local use\n",
    "4. Integrate into FastAPI (Phase 4)\n",
    "\n",
    "**Model Capabilities:**\n",
    "- Recognizes 8 entity types across 3 business domains\n",
    "- Cross-domain entity recognition\n",
    "- Based on RoBERTa transformer\n",
    "- Ready for production use"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
