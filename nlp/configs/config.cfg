# ============================================
# spaCy NER Training Configuration
# ============================================
# This file controls all aspects of model training
# Learn more: https://spacy.io/usage/training#config

[paths]
# Where to find training data
train = "nlp/training_data/spacy/train.spacy"
dev = "nlp/training_data/spacy/dev.spacy"

# Vectors: Pre-trained word embeddings (not needed for transformers)
vectors = null

# Initial model to start from (null = train from scratch)
init_tok2vec = null

[system]
# Random seed for reproducibility
# Same seed = same results every time
seed = 42

# GPU device (-1 = CPU, 0 = first GPU)
# Most laptops don't have compatible GPUs, so CPU is fine
gpu_allocator = null

[nlp]
# Language: English
lang = "en"

# Pipeline: Only NER (Named Entity Recognition)
# We could add other components like parser, but we only need NER
pipeline = ["transformer","ner"]

# Batch size: Number of examples processed together
# Larger = faster but needs more memory
# 128 is good for GPU, 32-64 for CPU
batch_size = 1000

# Don't disable any components during training
disabled = []

# Tokenizer: How text is split into words
# "spacy.Tokenizer.v1" is the standard tokenizer
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

# ============================================
# Transformer Component (RoBERTa)
# ============================================
# This is the "brain" that understands language context
[components.transformer]
factory = "transformer"

[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v3"

# Model name: RoBERTa-base from HuggingFace
# Why RoBERTa? 
# - Pre-trained on 160GB of text
# - Understands context (e.g., "Apple" company vs apple fruit)
# - Balance of speed and accuracy
name = "roberta-base"

# Mixed precision: Faster training with minimal accuracy loss
mixed_precision = false

# Tokenizer config
tokenizer_config = {"use_fast": true}

# Transformer-specific config
transformer_config = {}

# Get pooling: How to combine word pieces into word representations
[components.transformer.model.get_spans]
@span_getters = "spacy-transformers.strided_spans.v1"
window = 128
stride = 96

# ============================================
# NER Component (Entity Recognition)
# ============================================
[components.ner]
factory = "ner"

# Incorrect spans: What to do with overlapping entities
# "discard" = ignore problematic examples
incorrect_spans_key = null

# Moves: Transition-based parsing moves
# These control how the model builds entity spans
moves = null

# Scoring: How to measure performance
# F-score is the main metric
scorer = {"@scorers":"spacy.ner_scorer.v1"}

# Update entities: Whether to update existing entities
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v2"

# State type: How the model tracks parsing state
state_type = "ner"

# Extra state tokens: Additional context
extra_state_tokens = false

# Hidden width: Size of hidden layer
# 64 is sufficient for most NER tasks
hidden_width = 64

# Max positive: Maximum number of actions to consider
maxout_pieces = 2

# Use upper layer: Whether to use features from upper parsing layers
use_upper = false

# Number of layers in the model
nO = null

[components.ner.model.tok2vec]
@architectures = "spacy-transformers.TransformerListener.v1"

# Gradient propagation: How gradients flow back to transformer
grad_factor = 1.0

# Pooling: How to combine transformer outputs
[components.ner.model.tok2vec.pooling]
@layers = "reduce_mean.v1"

# ============================================
# Training Configuration
# ============================================
[corpora]

# Training data loader
[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
max_length = 0

# Augmentation: Data augmentation techniques (disabled for now)
[corpora.train.augmenter]
@augmenters = "spacy.orth_variants.v1"
level = 0.0
lower = 0.0
orth_variants = {}

# Development (validation) data loader
[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}
max_length = 0

# ============================================
# Training Loop
# ============================================
[training]

# Accumulation: Accumulate gradients over N batches
# Simulates larger batch size without memory cost
accumulate_gradient = 2

# Development corpus for validation
dev_corpus = "corpora.dev"

# Training corpus
train_corpus = "corpora.train"

# Random seed
seed = ${system.seed}

# GPU memory allocation
gpu_allocator = ${system.gpu_allocator}

# Dropout: Randomly disable neurons to prevent overfitting
# 0.1 = 10% of neurons disabled during training
dropout = 0.1

# Patience: Stop if no improvement after N steps
# 3000 steps ≈ 10-15 epochs for 800 sentences
patience = 3000

# Max epochs: Maximum number of full passes through data
# 0 = no limit (use max_steps instead)
max_epochs = 0

# Max steps: Stop after this many training steps
# 10000 steps ≈ 1-2 hours on CPU
max_steps = 10000

# Evaluation frequency: Validate every N steps
eval_frequency = 200

# Frozen components: Components not to train
frozen_components = []

# Annotating components: Components that add annotations
annotating_components = []

# Checkpoint frequency: Save model every N steps (disabled)
before_to_disk = null

# Before update: Run this callback before each update (disabled)
before_update = null

# ============================================
# Batch Size Configuration
# ============================================
[training.batcher]
@batchers = "spacy.batch_by_words.v1"

# Disable shuffling (we shuffle data manually)
discard_oversize = false

# Tolerance: How much variation in batch size is allowed
tolerance = 0.2

# Get length: How to measure example size
[training.batcher.size]
@schedules = "compounding.v1"

# Batch size grows from 50 to 500 words (reduced for stability)
# Smaller batches at start = more stable training
start = 50
stop = 500
compound = 1.001

# ============================================
# Optimizer (How model learns)
# ============================================
[training.optimizer]
@optimizers = "Adam.v1"

# Beta1, Beta2: Adam optimizer momentum parameters
beta1 = 0.9
beta2 = 0.999

# L2 regularization: Prevents overfitting
# Small penalty on large weights
L2_is_weight_decay = true
L2 = 0.01

# Gradient clipping: Prevents exploding gradients
grad_clip = 1.0

# Use averaging: Average weights over time (improves stability)
use_averages = false

# Epsilon: Small constant to prevent division by zero
eps = 1e-8

# ============================================
# Learning Rate Schedule
# ============================================
# Learning rate: How big are the update steps
# Starts at 5e-5 (0.00005) with warmup, then decreases linearly
[training.optimizer.learn_rate]
@schedules = "warmup_linear.v1"

# Warmup: Gradually increase learning rate at start
# Prevents early instability
warmup_steps = 250

# Total steps: Learning rate decreases to 0 by this step
total_steps = 20000

# Initial rate: Starting learning rate (very small)
initial_rate = 5e-5

# ============================================
# Scoring & Evaluation
# ============================================
[training.score_weights]
# F-score is the main metric (precision + recall combined)
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0

# Per-entity-type scores
ents_per_type = null

# ============================================
# Initialization
# ============================================
[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}

# Vocabulary: Start with empty vocabulary
vocab_data = null

# Lookups: Additional lookup tables (not needed)
lookups = null

# Before init: Callback before initialization
before_init = null

# After init: Callback after initialization
after_init = null

[initialize.components]

[initialize.tokenizer]
